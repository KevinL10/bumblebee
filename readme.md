# vit 

model implementation
- [x] load cifar-10 dataset and update training process (batching)
- [x] add learnable start token `[class]` & classify
- [ ] change classification head to MLP
- [ ] add layernorm & multi-head attention 
- [ ] add layernorm & mlp & repeat 
- [ ] benchmarking



compilation:
- [ ] compile to wasm (webgpu)
- [ ] add boilerplate index.html & js loader